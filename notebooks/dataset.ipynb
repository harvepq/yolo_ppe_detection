{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install roboflow==1.1.49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import roboflow\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Directory structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run local or not\n",
    "PROJ_ROOT = os.getcwd()\n",
    "is_local = True if os.path.basename(PROJ_ROOT) == \"notebooks\" else False\n",
    "\n",
    "# Directory path\n",
    "if not is_local:\n",
    "  os.makedirs(\"data\", exist_ok=True)\n",
    "  data_relative_path =\"data\"\n",
    "else:\n",
    "  data_relative_path =\"../data/processed\"\n",
    "\n",
    "PROCESSED_DATA_DIR = os.path.join(PROJ_ROOT, data_relative_path)\n",
    "\n",
    "# Print processed data path\n",
    "print(f\"\\033[34m INFO: Processed data in {PROCESSED_DATA_DIR}! \\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class\n",
    "class CFG:\n",
    "  # Clases\n",
    "  CLASSES = [\"hard_hat\",\n",
    "            \"no_hard_hat\",\n",
    "            \"no_safety_harness\",\n",
    "            \"no_safety_vest\",\n",
    "            \"person\",\n",
    "            \"safety_harness\",\n",
    "            \"safety_vest\"]\n",
    "\n",
    "  # Dataset settings (Roboflow)\n",
    "  DATASET_WORKSPACE = \"deeplearning-cwudo\"\n",
    "  DATASET_PROJECT = \"yolo_ppe_detection\"\n",
    "  DATASET_FORMAT = \"yolov8\"\n",
    "  DATASET_VERSION = 5\n",
    "  DATASET_NAME = f\"ppe_dataset_v{DATASET_VERSION}\"\n",
    "  DATASET_PATH = os.path.join(PROCESSED_DATA_DIR, DATASET_NAME)\n",
    "  DATASET_YAML_PATH = os.path.join(DATASET_PATH, \"data.yaml\")\n",
    "\n",
    "# Print information\n",
    "print(f\"\\nDataset information (Roboflow)\")\n",
    "print(f\"Dataset project: {CFG.DATASET_PROJECT}\")\n",
    "print(f\"Dataset version: {CFG.DATASET_VERSION}\")\n",
    "print(f\"Dataset path: {CFG.DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(CFG.DATASET_YAML_PATH):\n",
    "  # Login into roboflow\n",
    "  roboflow.login()\n",
    "\n",
    "  rf = roboflow.Roboflow()\n",
    "\n",
    "  project = rf.workspace(CFG.DATASET_WORKSPACE).project(CFG.DATASET_PROJECT)\n",
    "  version = project.version(CFG.DATASET_VERSION)\n",
    "  dataset = version.download(model_format=CFG.DATASET_FORMAT, location=CFG.DATASET_PATH, overwrite=True)\n",
    "else:\n",
    "  print(\"The dataset exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify dataset yaml file\n",
    "def modify_yaml_file(file_path):\n",
    "  with open(file_path, \"r+\") as file:\n",
    "    try:\n",
    "      data = yaml.safe_load(file)\n",
    "      file.seek(0)\n",
    "      file.truncate(0)\n",
    "\n",
    "      data[\"train\"] = os.path.join(CFG.DATASET_PATH, \"train/images\")\n",
    "      data[\"val\"] = os.path.join(CFG.DATASET_PATH, \"valid/images\")\n",
    "      data[\"test\"] = os.path.join(CFG.DATASET_PATH, \"test/images\")\n",
    "\n",
    "      yaml.dump(data, file)\n",
    "      yaml_data = yaml.dump(data, default_style=False)\n",
    "      print(yaml_data)\n",
    "\n",
    "    except yaml.YAMLError as e:\n",
    "      print(\"Error reading YAML: \", e)\n",
    "\n",
    "  file.close()\n",
    "\n",
    "# Print dataset yaml file\n",
    "modify_yaml_file(CFG.DATASET_YAML_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset image visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image function\n",
    "def display_image(image, print_info = True, hide_axis = False):\n",
    "  if isinstance(image, str): # Check if it's a file path\n",
    "    img = Image.open(image)\n",
    "    plt.imshow(img)\n",
    "  elif isinstance(image, np.ndarray): # Check if it's a NumPy array\n",
    "    image = image[..., ::-1] # BGR to RGB\n",
    "    img = Image.fromarray(image)\n",
    "    plt.imshow(img)\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported image format\")\n",
    "\n",
    "  if print_info:\n",
    "    print(\"Type: \", type(img), \"\\n\")\n",
    "    print(\"Shape: \", np.array(img).shape, \"\\n\")\n",
    "\n",
    "  if hide_axis:\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "  # Plot image\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one valid subset: train, test or valid\n",
    "subset = \"train\"\n",
    "images_path = os.path.join(CFG.DATASET_PATH, subset, \"images\")\n",
    "images_list = os.listdir(images_path)\n",
    "image_index = random.randint(0, len(images_list))\n",
    "\n",
    "# Random image path\n",
    "example_image_path = os.path.join(images_path, images_list[image_index])\n",
    "\n",
    "# Plot example image\n",
    "display_image(example_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization of many images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display many images\n",
    "def plot_random_images_from_folder(folder_path, num_images = 20, seed = 0):\n",
    "\n",
    "  random.seed(seed)\n",
    "\n",
    "  # Get a list of image files in the folder\n",
    "  image_files = [f for f in os.listdir(folder_path) if f.endswith((\".jpg\", \".png\", \".jpeg\", \".gif\"))]\n",
    "\n",
    "  # Make sure that we have a least num_images files to choose from\n",
    "  if len(image_files) < num_images:\n",
    "    raise ValueError(\"Not enough images in the folder\")\n",
    "\n",
    "  # Randomly select num_images image files\n",
    "  selected_files = random.sample(image_files, num_images)\n",
    "\n",
    "  # Create a subplot grid\n",
    "  num_cols = 5\n",
    "  num_rows = (num_images + num_cols - 1) // num_cols\n",
    "  fig, axes = plt.subplots(num_rows, num_cols, figsize=(12,10))\n",
    "\n",
    "  for i, file_name in enumerate(selected_files):\n",
    "    # Open and display the image using PIL\n",
    "    img = Image.open(os.path.join(folder_path, file_name))\n",
    "\n",
    "    if num_rows == 1:\n",
    "      ax = axes[i % num_cols]\n",
    "    else:\n",
    "      ax = axes[i // num_cols, i % num_cols]\n",
    "\n",
    "    ax.imshow(img)\n",
    "    # ax.axis(\"off\")\n",
    "    # ax.set_title(file_name)\n",
    "\n",
    "  # Remove empty subplots\n",
    "  for i in range(num_images, num_rows * num_cols):\n",
    "    if num_rows == 1:\n",
    "      fig.delaxes(axes[i % num_cols])\n",
    "    else:\n",
    "      fig.delaxes(axes[i // num_cols, i % num_cols])\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one valis subset: train, test or valid\n",
    "subset = \"train\"\n",
    "images_path = os.path.join(CFG.DATASET_PATH, subset, \"images\")\n",
    "plot_random_images_from_folder(images_path, num_images=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes\n",
    "class_idx = {str(i): CFG.CLASSES[i] for i in range(len(CFG.CLASSES))}\n",
    "class_stat = {}\n",
    "data_len = {}\n",
    "class_info = []\n",
    "\n",
    "for subset in [\"train\", \"valid\", \"test\"]:\n",
    "  class_count = {CFG.CLASSES[i]: 0 for i in range(len(CFG.CLASSES))}\n",
    "\n",
    "  labels_path = os.path.join(CFG.DATASET_PATH, subset, \"labels\")\n",
    "  for file in os.listdir(labels_path):\n",
    "    with open(os.path.join(labels_path, file)) as f:\n",
    "      lines = f.readlines()\n",
    "\n",
    "      for cls in [line[0] for line in lines]:\n",
    "        class_count[class_idx[cls]] += 1\n",
    "\n",
    "    f.close()\n",
    "\n",
    "  data_len[subset] = len(os.listdir(labels_path))\n",
    "  class_stat[subset] = class_count\n",
    "\n",
    "  class_info.append({\"Subset\": subset, **class_count, \"Data_Volume\": data_len[subset]})\n",
    "\n",
    "# Convert class info list to pandas dataframe\n",
    "dataset_stats_df = pd.DataFrame(class_info)\n",
    "dataset_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots with 1 row and 3 columns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "\n",
    "# Plot vertical bar plots for each mode in subplots\n",
    "for i, mode in enumerate([\"train\", \"valid\", \"test\"]):\n",
    "  sns.barplot(\n",
    "    data = dataset_stats_df[dataset_stats_df[\"Subset\"] == mode].drop(columns=\"Subset\"),\n",
    "    orient=\"v\",\n",
    "    ax = axes[i],\n",
    "    palette = \"Set2\"\n",
    "  )\n",
    "\n",
    "  axes[i].set_title(f\"{mode.capitalize()} Class Statistics\")\n",
    "  axes[i].set_xlabel(\"Classes\")\n",
    "  axes[i].set_ylabel(\"Count\")\n",
    "  axes[i].tick_params(axis = \"x\", rotation = 90)\n",
    "\n",
    "  # Add annotations on top of each bar\n",
    "  for p in axes[i].patches:\n",
    "    axes[i].annotate(f\"{int(p.get_height())}\",\n",
    "                      (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                      ha = \"center\",\n",
    "                      va = \"center\",\n",
    "                      fontsize = 8,\n",
    "                      color = \"black\",\n",
    "                      xytext = (0,5),\n",
    "                      textcoords = \"offset points\"\n",
    "                    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in [\"train\", \"valid\", \"test\"]:\n",
    "  print(f\"\\nImage size in {mode} set:\")\n",
    "\n",
    "  img_size = 0\n",
    "\n",
    "  for file in glob.glob(os.path.join(CFG.DATASET_PATH, mode, \"images\", \"*\")):\n",
    "\n",
    "    image = Image.open(file)\n",
    "\n",
    "    if image.size != img_size:\n",
    "      print(f\"{image.size}\")\n",
    "      img_size = image.size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
